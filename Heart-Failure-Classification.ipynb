{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1- We will start by data preparation and processing"
      ],
      "metadata": {
        "id": "l1pKqyc0cFPw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "v41PPrZluJMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4814d82-6a00-45c8-9072-081c5cae50bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "path = \"/content/drive/MyDrive/heart.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(df.head())\n",
        "X = df.drop(columns=['HeartDisease'])\n",
        "y = df['HeartDisease']\n",
        "np.random.seed(42)\n",
        "# Stratified Split: 70% Train, 10% Validation, 20% Test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, stratify=y_temp, random_state=42)\n",
        "\n",
        "\n",
        "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), [col for col in X_train.columns if col not in categorical_features]), # Apply StandardScaler to numerical features\n",
        "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features), # Apply OneHotEncoder to categorical features, sparse=False for numpy array\n",
        "    ])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_val = preprocessor.transform(X_val)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"Data prepared: Train =\", len(y_train), \", Val =\", len(y_val), \", Test =\", len(y_test))\n"
      ],
      "metadata": {
        "id": "9ZPLiesHuMvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e2464e-74e7-4a21-f792-74e731edcb7d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
            "0   40   M           ATA        140          289          0     Normal    172   \n",
            "1   49   F           NAP        160          180          0     Normal    156   \n",
            "2   37   M           ATA        130          283          0         ST     98   \n",
            "3   48   F           ASY        138          214          0     Normal    108   \n",
            "4   54   M           NAP        150          195          0     Normal    122   \n",
            "\n",
            "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
            "0              N      0.0       Up             0  \n",
            "1              N      1.0     Flat             1  \n",
            "2              N      0.0       Up             0  \n",
            "3              Y      1.5     Flat             1  \n",
            "4              N      0.0       Up             0  \n",
            "Data prepared: Train = 642 , Val = 92 , Test = 184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 - Implementation**"
      ],
      "metadata": {
        "id": "O7zea6fwc0wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement 3 different classifiers and compare between their results"
      ],
      "metadata": {
        "id": "4SKqWzSbc5Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 - Decision Tree**"
      ],
      "metadata": {
        "id": "2eFc3wnedAy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self,left=None,right=None,feature=None,threshold=None,*,value=None):\n",
        "        self.left=left\n",
        "        self.right=right\n",
        "        self.value=value\n",
        "        self.feature=feature\n",
        "        self.threshold=threshold\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.value is not None\n",
        "\n"
      ],
      "metadata": {
        "id": "UPWmBQ4GdGYP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self,max_depth=100,min_samples_split=2):\n",
        "        self.root=None\n",
        "        self.max_depth=max_depth\n",
        "        self.min_samples_split=min_samples_split\n",
        "\n",
        "    def fit(self,x,y):\n",
        "        self.n_feautures=x.shape[1]\n",
        "        self.root=self.grow_tree(x,y)\n",
        "\n",
        "    def grow_tree(self,x,y,depth=0):\n",
        "        n_samples,n_features=x.shape\n",
        "        n_labels=len(np.unique(y))\n",
        "\n",
        "        #check the stopping criteria\n",
        "        if(depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value=self.most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "        #find the best split\n",
        "        feat_indxs= np.random.choice(n_features,self.n_feautures,replace=False)\n",
        "        best_feat,best_thresh=self.best_split(x,y,feat_indxs)\n",
        "\n",
        "        #create child nodes\n",
        "        left_idxs,right_idxs=self.split(x[:,best_feat],best_thresh)\n",
        "        left=self.grow_tree(x[left_idxs,:],y[left_idxs],depth+1)\n",
        "        right=self.grow_tree(x[right_idxs,:],y[right_idxs],depth+1)\n",
        "        return Node(left,right,best_feat,best_thresh)\n",
        "\n",
        "    def best_split(self,x,y,feat_indxs):\n",
        "        best_gain=-1\n",
        "        best_feat,best_thresh=None,None\n",
        "\n",
        "        for feat_index in feat_indxs:\n",
        "            x_column=x[:,feat_index]\n",
        "            thresholds=np.unique(x_column)\n",
        "            for thr in thresholds:\n",
        "                gain=self.information_gain(y,x_column,thr)\n",
        "\n",
        "                if gain>best_gain:\n",
        "                    best_gain=gain\n",
        "                    best_feat=feat_index\n",
        "                    best_thresh=thr\n",
        "\n",
        "        return best_feat,best_thresh\n",
        "\n",
        "    def information_gain(self,y,x_column,threshold):\n",
        "        #parent entropy\n",
        "        parent_entropy=self.entropy(y)\n",
        "        #create children\n",
        "        left_idx,right_idx=self.split(x_column,threshold)\n",
        "        if len(left_idx)==0 or len(right_idx)==0:\n",
        "            return 0\n",
        "        #calculate the weighted avg.entropy of the children\n",
        "        n=len(y)\n",
        "        n_l,n_r=len(left_idx),len(right_idx)\n",
        "        e_l,e_r=self.entropy(y[left_idx]),self.entropy(y[right_idx])\n",
        "        child_entropy=(n_l/n)*e_l+(n_r/n)*e_r\n",
        "        #calculate the information gain\n",
        "        ig=parent_entropy-child_entropy\n",
        "        return ig\n",
        "\n",
        "    def split(self,x_column,threshold):\n",
        "        left_idxs=np.argwhere(x_column<=threshold).flatten()\n",
        "        right_idxs=np.argwhere(x_column>threshold).flatten()\n",
        "        return left_idxs,right_idxs\n",
        "\n",
        "    def entropy(self,y):\n",
        "        hist= np.bincount(y)\n",
        "        ps= hist/len(y)\n",
        "        return -np.sum([p*np.log2(p) for p in ps if p>0])\n",
        "\n",
        "    def most_common_label(self,y):\n",
        "        counter= Counter(y)\n",
        "        most_common=counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self,x):\n",
        "        return np.array([self.traverse_tree(xi,self.root) for xi in x])\n",
        "\n",
        "    def traverse_tree(self,x,node):\n",
        "        if node.is_leaf():\n",
        "            return node.value\n",
        "        if x[node.feature]<=node.threshold:\n",
        "            return self.traverse_tree(x,node.left)\n",
        "        return self.traverse_tree(x,node.right)\n",
        "    def accuracy(self,y_pred,y_test):\n",
        "        return np.sum(y_pred==y_test)/len(y_test)"
      ],
      "metadata": {
        "id": "X_bhmGuudbq6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will train the decision tree on different depth values and see the scores"
      ],
      "metadata": {
        "id": "I7OedKpUd1RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_depth = None\n",
        "best_acc = 0\n",
        "for depth in range(1, 101, 10):  # Example tuning loop\n",
        "    clf = DecisionTree(max_depth=depth)\n",
        "    clf.fit(X_train, y_train.to_numpy())\n",
        "    val_pred = clf.predict(X_val)\n",
        "    val_acc = clf.accuracy(val_pred, y_val.to_numpy())\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_depth = depth\n",
        "\n",
        "clf=DecisionTree(best_depth)\n",
        "clf.fit(np.vstack((X_train,X_val)), np.hstack((y_train,y_val)))\n",
        "test_pred = clf.predict(X_test)\n",
        "test_acc = clf.accuracy(test_pred, y_test.to_numpy())\n",
        "\n",
        "dt_f1 = f1_score(y_test, test_pred)\n",
        "\n",
        "print(f\"DT Accuracy: {test_acc:.4f}, F1-Score: {dt_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPdYoWfCeOfE",
        "outputId": "1af9cd95-2e68-4d93-853c-89a0bb7bc446"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642,) (92,)\n",
            "DT Accuracy: 0.8261, F1-Score: 0.8351\n",
            "Confusion Matrix:\n",
            " [[71 11]\n",
            " [21 81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 - Bagging**"
      ],
      "metadata": {
        "id": "QZCqSKM6jLaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bagging:\n",
        "    def __init__(self, base_learner, n_estimators, sample_size=0.8):\n",
        "        self.base_learner = base_learner\n",
        "        self.n_estimators = n_estimators\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def fit(self,x,y):\n",
        "        n_samples=int(self.sample_size*len(x))\n",
        "        self.models=[]\n",
        "        for i in range(self.n_estimators):\n",
        "            indxs=np.random.choice(len(x),n_samples,replace=True)\n",
        "            x_sample,y_sample=x[indxs],y[indxs]\n",
        "            model = DecisionTree()\n",
        "            model.fit(x_sample,y_sample)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self,x):\n",
        "        predictions=np.array([model.predict(x) for model in self.models])\n",
        "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "    def accuracy(self,y_pred,y_test):\n",
        "        return np.sum(y_pred==y_test)/len(y_test)"
      ],
      "metadata": {
        "id": "XkDeXAyvjRic"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will train the bagging algorithm and calculate the accuracy and score\n"
      ],
      "metadata": {
        "id": "-K-x65ghjbwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimators = None\n",
        "best_acc = 0\n",
        "for estimators in range(10, 20, 30):\n",
        "    clf = Bagging(base_learner=DecisionTree(), n_estimators=estimators)\n",
        "    clf.fit(X_train, y_train.to_numpy())\n",
        "    val_pred = clf.predict(X_val)\n",
        "    val_acc = clf.accuracy(val_pred, y_val.to_numpy())\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_estimators = estimators\n",
        "\n",
        "clf=Bagging(base_learner=DecisionTree(), n_estimators=best_estimators)\n",
        "clf.fit(np.vstack((X_train,X_val)), np.hstack((y_train,y_val)))\n",
        "test_pred = clf.predict(X_test)\n",
        "bagging_acc = clf.accuracy(test_pred, y_test.to_numpy())\n",
        "\n",
        "bagging_f1 = f1_score(y_test, test_pred)\n",
        "\n",
        "print(f\"Bagging Accuracy: {bagging_acc:.4f}, F1-Score: {bagging_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUDxyspEjxH2",
        "outputId": "22c21aa8-ee0f-4da9-f239-b12d36ce82cf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.8696, F1-Score: 0.8763\n",
            "Confusion Matrix:\n",
            " [[75  7]\n",
            " [17 85]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then finally, We will implement the ADAboost classifier"
      ],
      "metadata": {
        "id": "ER-V3oHzuAPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class decisionStump:\n",
        "    def __init__(self):\n",
        "        self.polarity = 1\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.alpha = None\n",
        "\n",
        "    def predict(self,X):\n",
        "        n_samples = X.shape[0]\n",
        "        X_column = X[:,self.feature_index]\n",
        "        predictions = np.ones(n_samples)\n",
        "        if self.polarity == 1:\n",
        "            predictions[X_column < self.threshold]= -1\n",
        "        else:\n",
        "            predictions[X_column > self.threshold]= -1\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "QNPIdBweuHhh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class adaboost:\n",
        "    def __init__(self,n_clf=50):\n",
        "        self.n_clf = n_clf\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        n_samples,n_features = X.shape\n",
        "        #initialize weights\n",
        "        w = np.full(n_samples, (1/n_samples))\n",
        "        self.clfs = []\n",
        "\n",
        "        for i in range(self.n_clf):\n",
        "            clf= decisionStump()\n",
        "            min_error = float('inf')\n",
        "            for feature_i in range(n_features):\n",
        "                X_column =X[:,feature_i]\n",
        "                thresholds = np.unique(X_column)\n",
        "                for threshold in (thresholds):\n",
        "                    p=1\n",
        "                    predictions = np.ones(n_samples)\n",
        "                    predictions[X_column < threshold] = -1\n",
        "                    misclassified = w[y!=predictions]\n",
        "                    error = sum(misclassified)\n",
        "                    if error > 0.5:\n",
        "                        error = 1-error\n",
        "                        p = -1\n",
        "\n",
        "                    if error < min_error:\n",
        "                        min_error = error\n",
        "                        clf.polarity = p\n",
        "                        clf.threshold = threshold\n",
        "                        clf.feature_index = feature_i\n",
        "            eps=1e-10\n",
        "            clf.alpha = 0.5*np.log((1.0-min_error)/(min_error+eps))\n",
        "\n",
        "            predictions = clf.predict(X)\n",
        "\n",
        "            w *= np.exp(-clf.alpha*y*predictions)\n",
        "            w /= np.sum(w)\n",
        "\n",
        "            self.clfs.append(clf)\n",
        "\n",
        "    def predict(self,X):\n",
        "        clf_preds = [clf.alpha*clf.predict(X) for clf in self.clfs]\n",
        "        ypred = np.sum(clf_preds,axis=0)\n",
        "        return np.sign(ypred)\n",
        "    def accuracy(self,y_pred,y_test):\n",
        "        return np.sum(y_pred==y_test)/len(y_test)\n",
        ""
      ],
      "metadata": {
        "id": "LkJP2jTUuPmK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And again, the training loop."
      ],
      "metadata": {
        "id": "rkZqS3TjuTaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimators = None\n",
        "best_acc = 0\n",
        "for estimators in range(10, 20, 30):\n",
        "    clf = adaboost(n_clf=estimators)\n",
        "    clf.fit(X_train,  np.where(y_train==0,-1,1))\n",
        "    val_pred = clf.predict(X_val)\n",
        "    val_pred = np.where(val_pred==-1,0,1)\n",
        "    val_acc = clf.accuracy(val_pred, y_val.to_numpy())\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_estimators = estimators\n",
        "\n",
        "clf=adaboost(best_estimators)\n",
        "temp =  np.hstack((y_train,y_val))\n",
        "clf.fit(np.vstack((X_train,X_val)),np.where(temp==0,-1,1))\n",
        "test_pred = clf.predict(X_test)\n",
        "test_pred = np.where(test_pred==-1,0,1)\n",
        "ada_acc = clf.accuracy(test_pred, y_test.to_numpy())\n",
        "\n",
        "ada_f1 = f1_score(y_test, test_pred)\n",
        "\n",
        "print(f\"Adaboost Accuracy: {ada_acc:.4f}, F1-Score: {ada_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm3db1cIuRd1",
        "outputId": "0de3dca0-ecf9-430e-8f5e-4c329a5bd871"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaboost Accuracy: 0.8478, F1-Score: 0.8557\n",
            "Confusion Matrix:\n",
            " [[73  9]\n",
            " [19 83]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-Bonus part"
      ],
      "metadata": {
        "id": "y4J6vLnIuQD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will Start by preparing the data and splitting it into train, validation and test"
      ],
      "metadata": {
        "id": "rKl5Vy7TuwPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wne5TwnEuOY_",
        "outputId": "f385ff63-91c9-41e0-aea9-fe83c6a5f6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "path = \"/content/drive/MyDrive/heart.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "dPRf__HSvmKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c98b5e4-3e69-414b-cc0f-8666806e6efc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
            "0   40   M           ATA        140          289          0     Normal    172   \n",
            "1   49   F           NAP        160          180          0     Normal    156   \n",
            "2   37   M           ATA        130          283          0         ST     98   \n",
            "3   48   F           ASY        138          214          0     Normal    108   \n",
            "4   54   M           NAP        150          195          0     Normal    122   \n",
            "\n",
            "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
            "0              N      0.0       Up             0  \n",
            "1              N      1.0     Flat             1  \n",
            "2              N      0.0       Up             0  \n",
            "3              Y      1.5     Flat             1  \n",
            "4              N      0.0       Up             0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here comes the splitting and standardizing"
      ],
      "metadata": {
        "id": "wtaFYm-eww0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df.drop(columns=['HeartDisease'])\n",
        "y = df['HeartDisease']\n",
        "\n",
        "# Stratified Split: 70% Train, 10% Validation, 20% Test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, stratify=y_temp, random_state=42)\n",
        "\n",
        "\n",
        "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), [col for col in X_train.columns if col not in categorical_features]), # Apply StandardScaler to numerical features\n",
        "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features), # Apply OneHotEncoder to categorical features, sparse=False for numpy array\n",
        "    ])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_val = preprocessor.transform(X_val)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"Data prepared: Train =\", len(y_train), \", Val =\", len(y_val), \", Test =\", len(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnuaKXoOwwK7",
        "outputId": "72a1abd1-105a-4c83-c3fd-6f4d5e110cf7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data prepared: Train = 642 , Val = 92 , Test = 184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preparing and preprocessing the non-numerical value using one-hot encoding, we will begin by training a KNN model using different values for k."
      ],
      "metadata": {
        "id": "KFteJ62vzwz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "best_k = None\n",
        "best_val = 0\n",
        "\n",
        "for k in [3, 5, 7, 11,13,15]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    val = accuracy_score(y_val, knn.predict(X_val))\n",
        "\n",
        "    print(f\"K={k}, accuracy: {val:.4f}\")\n",
        "\n",
        "    if val > best_val:\n",
        "        best_val = val\n",
        "        best_k = k\n",
        "\n",
        "knn_final = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn_final.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))\n",
        "\n",
        "y_pred_knn = knn_final.predict(X_test)\n",
        "knn_acc = accuracy_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"KNN Accuracy: {knn_acc:.4f}, F1-Score: {knn_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Eba_syz710",
        "outputId": "f5fccf2c-13da-48fa-fe5a-5315cb7e6432"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K=3, accuracy: 0.8696\n",
            "K=5, accuracy: 0.8913\n",
            "K=7, accuracy: 0.9130\n",
            "K=11, accuracy: 0.9348\n",
            "K=13, accuracy: 0.9022\n",
            "K=15, accuracy: 0.9130\n",
            "KNN Accuracy: 0.8967, F1-Score: 0.9073\n",
            "Confusion Matrix:\n",
            " [[72 10]\n",
            " [ 9 93]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, We are implementing Logistic Regression"
      ],
      "metadata": {
        "id": "kDjN1lGz4e2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "best_C = None\n",
        "best_val = 0\n",
        "\n",
        "for C in [0.01, 0.1, 1, 10]:\n",
        "    log_reg = LogisticRegression(C=C, max_iter=1000, class_weight='balanced', random_state=42)\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    val = accuracy_score(y_val, log_reg.predict(X_val))\n",
        "\n",
        "    print(f\"C={C}, Accuracy: {val:.4f}\")\n",
        "\n",
        "    if val > best_val:\n",
        "        best_val = val\n",
        "        best_C = C\n",
        "\n",
        "log_reg_final = LogisticRegression(C=best_C, max_iter=1000, class_weight='balanced', random_state=42)\n",
        "log_reg_final.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))\n",
        "\n",
        "y_pred_log = log_reg_final.predict(X_test)\n",
        "log_acc = accuracy_score(y_test, y_pred_log)\n",
        "log_f1 = f1_score(y_test, y_pred_log)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {log_acc:.4f}, F1-Score: {log_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MVo1-_q4j6n",
        "outputId": "687a7eab-8456-4698-eb98-de158633ba3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.01, Accuracy: 0.9022\n",
            "C=0.1, Accuracy: 0.9022\n",
            "C=1, Accuracy: 0.9022\n",
            "C=10, Accuracy: 0.8913\n",
            "Logistic Regression Test Accuracy: 0.8641, F1-Score: 0.8744\n",
            "Confusion Matrix:\n",
            " [[72 10]\n",
            " [15 87]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, We get to the Feedforward NN"
      ],
      "metadata": {
        "id": "R2lxbC_X5wbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = FNN(X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):  # Train for 100 epochs\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_fnn = model(X_test_tensor).numpy().round()\n",
        "    fnn_acc = accuracy_score(y_test, y_pred_fnn)\n",
        "    fnn_f1 = f1_score(y_test, y_pred_fnn)\n",
        "\n",
        "print(f\"FNN Test Accuracy: {fnn_acc:.4f}, F1-Score: {fnn_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_fnn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlQf6DA854f9",
        "outputId": "cfe712fd-68d2-4a8f-d3d9-65421c86f9a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7321\n",
            "Epoch 50, Loss: 0.2943\n",
            "Epoch 100, Loss: 0.2397\n",
            "Epoch 150, Loss: 0.2016\n",
            "Epoch 200, Loss: 0.1588\n",
            "Epoch 250, Loss: 0.1193\n",
            "Epoch 300, Loss: 0.0882\n",
            "Epoch 350, Loss: 0.0651\n",
            "Epoch 400, Loss: 0.0486\n",
            "Epoch 450, Loss: 0.0355\n",
            "FNN Test Accuracy: 0.8315, F1-Score: 0.8410\n",
            "Confusion Matrix:\n",
            " [[71 11]\n",
            " [20 82]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is the comparison between the three of them:"
      ],
      "metadata": {
        "id": "C6h17lafF224"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"KNN - Accuracy: {knn_acc:.4f}, F1-Score: {knn_f1:.4f}\")\n",
        "print(f\"Logistic Regression - Accuracy: {log_acc:.4f}, F1-Score: {log_f1:.4f}\")\n",
        "print(f\"FNN - Accuracy: {fnn_acc:.4f}, F1-Score: {fnn_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AndSje7ZF9cB",
        "outputId": "14162c31-05ad-4667-b88a-6275cf61cdcd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN - Accuracy: 0.8967, F1-Score: 0.9073\n",
            "Logistic Regression - Accuracy: 0.8641, F1-Score: 0.8744\n",
            "FNN - Accuracy: 0.8315, F1-Score: 0.8410\n"
          ]
        }
      ]
    }
  ]
}